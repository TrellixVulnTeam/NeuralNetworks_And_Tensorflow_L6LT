{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guidance from work by **Trask** from [here](https://iamtrask.github.io/2015/07/12/basic-python-network/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Firstly, Implementation of a Simple two layered (Input and Output) **Artificial Neural Network**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nn1](nn1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating **Sigmoid** Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the desirable properties of a sigmoid function is that its output can be used to create its derivative. If the sigmoid's output is a variable **out**, then the derivative is simply **out * (1-out)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0,1],\n",
    "              [0,1,1],\n",
    "              [1,0,1],\n",
    "              [1,1,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Data\n",
    "\n",
    " After the transpose, this y matrix has 4 rows with one column as one should expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([[0,0,1,1]]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use random seed to have deterministic calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the Weights randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**W** here is the first layer of weights connecting layer 1 of network to layer 2.  \n",
    "Here since there are only two layers, we will need only a single weight matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We here keep the weights's mean to **zero** for mathematical reasons of initializing weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 2*np.random.random((3,1)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering,\n",
    "* l1 as the first layer (the inputs)\n",
    "* Dot product as 1 to 1 multiplication to get the same dim. matrix o/p."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use **Full Batch Training** which means feeding all of our training examples (here four) all at once for several iterartions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(10000):\n",
    "    #forward propagation\n",
    "    l1 = X\n",
    "    l2 = sigmoid(np.dot(l1,w)) #input times weight activate.\n",
    "  \n",
    "    #Calculating error\n",
    "    l2_err = y - l2\n",
    "    \n",
    "    #Calculating delta\n",
    "    #we multiply the \"slopes\" by the error, we are reducing the error of high confidence predictions.\n",
    "    l2_delta = l2_err * sigmoid(l2, True)\n",
    "    \n",
    "    #Updating the weights\n",
    "    w  = w + np.dot(l1.T, l2_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00966779],\n",
       "       [0.00786453],\n",
       "       [0.99358992],\n",
       "       [0.99211751]])"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe these are almost equal to y = [ [0, 0, 1, 1] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Second, Implementation of a Three layered (Input, Hidden and Output) **Artificial Neural Network**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nn2](nn2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the sigmoid function as already defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0,1],\n",
    "            [0,1,1],\n",
    "            [1,0,1],\n",
    "            [1,1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([[0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = 2*np.random.random((3,4)) - 1 #weights between input and hidden\n",
    "w2 = 2*np.random.random((4,1)) - 1 #weights between hidden and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.49641003190272537\n",
      "Error: 0.008584525653247159\n",
      "Error: 0.0057894598625078085\n",
      "Error: 0.004629176776769985\n",
      "Error: 0.003958765280273649\n",
      "Error: 0.0035101225678616766\n"
     ]
    }
   ],
   "source": [
    "for j in range(60000):\n",
    "    \n",
    "    #forward propagating\n",
    "    l1 = X\n",
    "    l2 = sigmoid(np.dot(l1, w1))\n",
    "    l3 = sigmoid(np.dot(l2, w2))\n",
    "    \n",
    "    #Error\n",
    "    l3_err = y - l3\n",
    "    \n",
    "    #To output error at different intervals\n",
    "    if(j % 10000 == 0):\n",
    "        print(\"Error: \"+str(np.mean(np.abs(l3_err))))\n",
    "    \n",
    "    l3_delta = l3_err * sigmoid(l3, True)\n",
    "    \n",
    "    # how much did each l2 value contribute to the l3 error \n",
    "    l2_err = l3_delta.dot(w2.T)\n",
    "    \n",
    "    l2_delta = l2_err * sigmoid(l2, True)\n",
    "\n",
    "    #updating the weights\n",
    "    w2 = w2 + np.dot(l2.T, l3_delta)\n",
    "    w1 = w1 + np.dot(l1.T, l2_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**l2_err = l3_delta.dot(w2.T)**\n",
    "\n",
    "Uses the **confidence weighted error** from l3 to establish an error for l2. To do this, it simply sends the error across the weights from l3 to l2. This gives what you could call a **contribution weighted error** because we learn how much each node value in l2 \"contributed\" to the error in l3. This step is called **backpropagating** and is the namesake of the algorithm. We then update w1 using the same steps we did in the 2 layer implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily observe the error decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00260572],\n",
       "       [0.99672209],\n",
       "       [0.99701711],\n",
       "       [0.00386759]])"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe these are almost equal to y = [ [0, 1, 1, 0] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
