# -*- coding: utf-8 -*-
"""tf_autoencoder_pca.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/125fNGjrjQsObik1DydRoq1uPC7zD-v-b

# A Simple Autoencoder for PCA

[Prashant Brahmbhatt](www.github.com/hashbanger)

____

### Importing
"""

import numpy as np
import matplotlib.pyplot as plt

"""First we create a made up dataset of three dimension ad reduce it tot two dimensions."""

from sklearn.datasets import make_blobs

data = make_blobs(n_samples= 100, n_features= 3, centers= 2, random_state = 101)

data

"""## Scaling the Data"""

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

train_full = scaler.fit_transform(data[0])

x_feat = train_full[:, 0]
y_feat = train_full[:, 1]
z_feat = train_full[:, 2]

from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure()
ax = fig.add_subplot(111, projection = '3d')

fig = plt.figure()
ax = fig.add_subplot(111, projection = '3d')
ax.scatter(x_feat, y_feat, z_feat, c = data[1])

"""## The Linear Autoencoder"""

import tensorflow as tf
from tensorflow.contrib.layers import fully_connected

num_inputs = 3 # three dimensional input
num_hidden = 2 # two dimensional representation
num_outputs = num_inputs

learning_rate = 0.01

"""### Placeholder"""

X = tf.placeholder(tf.float32, shape = [None, num_inputs])

"""### Layers

We do not provide any activation function using the layers API
"""

hidden = fully_connected(X, num_hidden, activation_fn = None)
outputs = fully_connected(hidden, num_outputs, activation_fn = None )

"""### Loss Function"""

loss = tf.reduce_mean(tf.square(outputs - X)) # Mean Squared Error

"""### Optimizer"""

optimizer = tf.train.AdamOptimizer(learning_rate)
train = optimizer.minimize(loss)

"""### Init"""

init  = tf.global_variables_initializer()

"""### Session"""

num_steps = 1000
with tf.Session() as sess:
    sess.run(init)
    
    for iteration in range(num_steps):
        sess.run(train, feed_dict = {X: train_full})
        
    # Now asking for the hidden layer output
    output_2d = hidden.eval(feed_dict = {X: train_full})

output_2d.shape

plt.scatter(output_2d[:, 0], output_2d[:, 1], c = data[1])

"""So we have successfully reduced the dimensions from three to two

### de nada!
"""