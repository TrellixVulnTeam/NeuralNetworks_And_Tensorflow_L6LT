{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN in Tensorflow.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "vwwjeMHChBc8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# A Convolutional Neural Network in Tensorflow"
      ]
    },
    {
      "metadata": {
        "id": "pDmgYdm-hBdG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[Prashant Brahmbhatt](www.github.com/hashbanger)"
      ]
    },
    {
      "metadata": {
        "id": "ndxiNkOFhBdO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Classifying MNIST dataset.  \n",
        "Detailed Account of each step."
      ]
    },
    {
      "metadata": {
        "id": "2xX_TXGUhBdW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "_____"
      ]
    },
    {
      "metadata": {
        "id": "O7RCck7PhBda",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Getting the data"
      ]
    },
    {
      "metadata": {
        "id": "a_vpFeT4hBdi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tw291BuuhBeD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        },
        "outputId": "50535ba1-c578-4c6c-b0f0-ecc83532959b"
      },
      "cell_type": "code",
      "source": [
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-2-a839aeb82f4b>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MJCdeZyLhcoA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "e9a1ee61-b40f-49bf-ee22-c8ab6738b674"
      },
      "cell_type": "code",
      "source": [
        "print(\"Size of:\")\n",
        "print(\"- Training-set:\\t\\t{}\".format(len(mnist.train.labels)))\n",
        "print(\"- Test-set:\\t\\t{}\".format(len(mnist.test.labels)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of:\n",
            "- Training-set:\t\t55000\n",
            "- Test-set:\t\t10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wOWQlZkkhBeW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Hyperparameters that will be required"
      ]
    },
    {
      "metadata": {
        "id": "-qFXkRN0hBea",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learning_rate = 0.0001\n",
        "epochs = 10\n",
        "batch_size = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "56yhapOPhBet",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we declare the training data placeholders"
      ]
    },
    {
      "metadata": {
        "id": "ZPeeysuKhBe4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "x : Since the images in this data are 28 * 28, after flattened input required will be of size  [ batch_size, 784 ]  "
      ]
    },
    {
      "metadata": {
        "id": "R_Vr-B4zhBfD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = tf.placeholder(tf.float32, [None, 784])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lp6XKAeQhBf8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Before we can use this data in the **TensorFlow convolution** and **pooling** functions, such as **conv2d()** and **max_pool()**   we need to reshape the data as these functions take **4D** data only.   \n",
        "The format of the data to be supplied is **[i, j, k, l]** where   \n",
        "\n",
        "**i** is the number of training samples,   \n",
        "\n",
        "**j** is the height of the image,   \n",
        "\n",
        "**k** is the weight and   \n",
        "\n",
        "**l** is the channel number.  \n",
        "\n",
        "Because we have a greyscale image, **l** will always be equal to 1 (if we had an **RGB image**, it would be equal to **3**).  \n",
        "\n",
        "Since we don’t know the size of the first dimension of x, so we don’t know what **i** is.  However, **tf.reshape()** allows us to put -1 in place of i and it will dynamically reshape based on the number of training samples as the training is performed. "
      ]
    },
    {
      "metadata": {
        "id": "Nv9sdEbihBgD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_shaped = tf.reshape(x, [-1, 28, 28, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zWTUhQLdhBga",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we declare output data placeholder.  \n",
        "\n",
        "for y : Since we used one hot encoder, there will be 10 columns, one for each digit."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "l4NCjgc-xmZK",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y = tf.placeholder(tf.float32, [None, 10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m_wibxbHhBhb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We have to use mnist.train.next_batch() to extract the digits labels as a one-hot vector   \n",
        "a digit of “3” will be represented in one-hot encoding as **[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]**"
      ]
    },
    {
      "metadata": {
        "id": "Gfcj9RLQhBhg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setting up the Convolutional Layer"
      ]
    },
    {
      "metadata": {
        "id": "tj5XIwIYhBhp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We have to implement the following: \n",
        "    \n",
        "- To hold the shape of the weights that determine the behaviour of the **5×5 **convolutional filter.  The format that the **conv2d()** function receives for the filter is: **[filter_height, filter_width, in_channels, out_channels].**  The height and width of the filter are provided in the **filter_shape** variables (in this case **[5, 5]**).  \n",
        "\n",
        " **Input Channels** for first layer it will be 1, but in further layers if will be depending upon previous layer output so will become 32 for second layer.  \n",
        " \n",
        "\n",
        "- Set up **weights** and **biases** as per the required dimensions. We use Normal Distribution.  \n",
        "\n",
        "\n",
        "- Now we setup convolutional layer, using **tf.nn.conv2d()** , whose first two arguments are input_data and weights,\n",
        "  The size of the weights tensor shows TensorFlow what size the convolutional filter should be.  \n",
        "  third argument is the **strides**. Its format is **[st, sx, sy, sc]**. where,  \n",
        "  \n",
        "  **sx**: stride in x direction (using 1 here)  \n",
        "  \n",
        "  **sy**: stride along y axis (using 1 here)  \n",
        "  \n",
        "  **st** and **sc** : always are used 1 or we would move the filter between training examples and between channels which is undesirable.\n",
        " \n",
        " \n",
        "-  The final parameter is the padding. Padding determines the output size of each channel and when it is set to **“SAME”** it produces dimensions of:  \n",
        "\n",
        "  **out_height = ceil( float ( in_height ) / float ( strides[1] ) )  \n",
        "  out_width  = ceil( float ( in_width ) / float ( strides[2] ) )**\n",
        "  \n",
        "  So, the output dims remain same as input, otherwise for 28 \\* 28 images output would be **(x-n+1,y-m+1)** = 24 \\* 24.\n",
        "  \n",
        "  \n",
        "  \n",
        "- Add **bias** and **activate**.\n",
        "\n",
        "\n",
        "- **Max Pooling**, with the initial arg as the convolution output layer, **ksize** is the size of the pooling window and the **strides** is the striding argument same as previously defined. In pooling we use strides of 2.  \n",
        " As the above fomula, if we use **SAME** padding and stride of 2 will cut our dimensions to just half."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "V3bPXEgOxpyC",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_new_conv_layer(input_data, num_input_channels, num_filters, filter_shape, pool_shape, name):\n",
        "    # the input shape to be used in tf.nn.conv2d()\n",
        "    conv_filt_shape = [filter_shape[0], filter_shape[1], num_input_channels,\n",
        "                      num_filters]\n",
        "\n",
        "    # initialization of the bias and the weights for the filter\n",
        "    weights = tf.Variable(tf.truncated_normal(conv_filt_shape, stddev=0.03),\n",
        "                                      name=name+'_W')\n",
        "    bias = tf.Variable(tf.truncated_normal([num_filters]), name=name+'_b')\n",
        "\n",
        "    # forming a covolution layer\n",
        "    out_layer = tf.nn.conv2d(input_data, weights, [1, 1, 1, 1], padding='SAME')\n",
        "\n",
        "    # adding the bias\n",
        "    out_layer += bias\n",
        "\n",
        "    # applying a ReLU non-linear activation\n",
        "    out_layer = tf.nn.relu(out_layer)\n",
        "\n",
        "    # the max pooling operation on the activated output layer\n",
        "    ksize = [1, pool_shape[0], pool_shape[1], 1]\n",
        "    strides = [1, 2, 2, 1]\n",
        "    out_layer = tf.nn.max_pool(out_layer, ksize=ksize, strides=strides, \n",
        "                               padding='SAME')\n",
        "\n",
        "    return out_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qnvGPa-RhBiC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Creating two convolution layers"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "0-yXICSSyFtn",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "layer1 = create_new_conv_layer(x_shaped, 1, 32, [5, 5], [2, 2], name='layer1')\n",
        "layer2 = create_new_conv_layer(layer1, 32, 64, [5, 5], [2, 2], name='layer2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9rNQNOL4hBiV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The Fully Connected Layers"
      ]
    },
    {
      "metadata": {
        "id": "z4KhuXIvhBil",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now first we have to **flatten** out our output of pooling.  \n",
        "Since we have pooled two times the image dimensions have been halved twice."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "mQJqa1CmyQ8F",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "flattened = tf.reshape(layer2, [-1, 7 * 7 * 64])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AW1MLGIahBi7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Initializing the **weights** and **bias** for the neural net, with 1000 nodes in the hidden layer.  \n",
        "We initialise the values of the weights using a **random normal distribution** with a **mean of zero** and a standard deviation of **0.03**.    \n",
        "Then activating."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jcEQU9NFyS_D",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# setup some weights and bias values for this layer, then activate with ReLU\n",
        "wd1 = tf.Variable(tf.truncated_normal([7 * 7 * 64, 1000], stddev=0.03), name='wd1')\n",
        "bd1 = tf.Variable(tf.truncated_normal([1000], stddev=0.01), name='bd1')\n",
        "dense_layer1 = tf.matmul(flattened, wd1) + bd1\n",
        "dense_layer1 = tf.nn.relu(dense_layer1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NvFt2ZVJhBjF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we define second dense layer.  \n",
        "In Tensorflow **Logits** is a name that it is thought to imply that this Tensor is the quantity that is being mapped to probabilities by the Softmax function."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "LfgPw-JRyVB6",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# another layer with softmax activations\n",
        "wd2 = tf.Variable(tf.truncated_normal([1000, 10], stddev=0.03), name='wd2')\n",
        "bd2 = tf.Variable(tf.truncated_normal([10], stddev=0.01), name='bd2')\n",
        "dense_layer2 = tf.matmul(dense_layer1, wd2) + bd2\n",
        "y_ = tf.nn.softmax(dense_layer2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jimK-oKwhBjl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The Cross Entropy Cost Function"
      ]
    },
    {
      "metadata": {
        "id": "178QCRYOhBjp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here we now use the cross entropy function denoted as :"
      ]
    },
    {
      "metadata": {
        "id": "-YjSMBqxhBjs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### $$J = -\\frac{1}{m} \\sum_{i=1}^m \\sum_{j=1}^n y_j^{(i)}log(y_j\\_^{(i)}) + (1 – y_j^{(i)})log(1 – y_j\\_^{(i)})$$"
      ]
    },
    {
      "metadata": {
        "id": "dIVnt8lhhBjz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Where \n",
        "$ y_j^{(i)} $ is the ith training label for output node j,   \n",
        "$y_j\\_^{(i)}$ is the ith predicted label for output node j,  \n",
        "**m** is the number of training / batch samples and **n** is the number of output nodes"
      ]
    },
    {
      "metadata": {
        "id": "suM2hfYThBj6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TensorFlow provides a handy function which applies soft-max followed by cross-entropy loss.  \n",
        "\n",
        " The function first takes the soft-max of the matrix multiplication, then compares it to the training target using cross-entropy.  The result is the cross-entropy calculation per training sample, so we need to reduce this tensor into a scalar (a single value).  To do this we use **tf.reduce_mean()** which takes a mean of the tensor."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "QYHwLfdmyW6d",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=dense_layer2, labels=y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G6ZjCYJzhBkj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training the Neural Net"
      ]
    },
    {
      "metadata": {
        "id": "SY9BT4rFhBks",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using the **Mini-Batch Gradient Descent** approach to train. we proceed as ;  \n",
        "\n",
        "\n",
        "- Create an optimiser  \n",
        "    We are using the Adam optimizer learn about that [here](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)\n",
        "- Create correct prediction and accuracy evaluation operations\n",
        "- Initialise the operations\n",
        "- Determine the number of batch runs within an training epoch\n",
        "  - For each epoch\n",
        "    * For each batch\n",
        "          - Extract the batch data  \n",
        "          - Run the optimiser and cross-entropy operations  \n",
        "          - Add to the average cost    \n",
        "         \n",
        "         \n",
        "     * Calculate the current test accuracy\n",
        "     * Print out some results\n",
        "- Calculate the final test accuracy and print\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "DKB7wibnyZgk",
        "outputId": "cd5d4a1b-507d-4884-c687-2a612310e038",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "cell_type": "code",
      "source": [
        "# creating an optimizer\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
        "\n",
        "# operations to determine accuracy \n",
        "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "# setup the initialisation operator\n",
        "init_op = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    # initialise the variables\n",
        "    sess.run(init_op)\n",
        "    \n",
        "    total_batch = int(len(mnist.train.labels) / batch_size)\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        avg_cost = 0\n",
        "        \n",
        "        for i in range(total_batch):\n",
        "            batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)\n",
        "            \n",
        "            _, c = sess.run([optimizer, cross_entropy], \n",
        "                            feed_dict={x: batch_x, y: batch_y})\n",
        "            #We don't care much about output from optimizer so assigned it to _\n",
        "            \n",
        "            avg_cost += c / total_batch\n",
        "            \n",
        "        test_acc = sess.run(accuracy, \n",
        "                       feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
        "        \n",
        "        print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(avg_cost), \"test accuracy: {:.3f}\".format(test_acc))\n",
        "\n",
        "    print(\"\\nTraining complete!\")\n",
        "    print(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 cost = 0.712 test accuracy: 0.939\n",
            "Epoch: 2 cost = 0.156 test accuracy: 0.969\n",
            "Epoch: 3 cost = 0.099 test accuracy: 0.978\n",
            "Epoch: 4 cost = 0.072 test accuracy: 0.977\n",
            "Epoch: 5 cost = 0.058 test accuracy: 0.983\n",
            "Epoch: 6 cost = 0.048 test accuracy: 0.985\n",
            "Epoch: 7 cost = 0.041 test accuracy: 0.987\n",
            "Epoch: 8 cost = 0.035 test accuracy: 0.989\n",
            "Epoch: 9 cost = 0.030 test accuracy: 0.987\n",
            "Epoch: 10 cost = 0.026 test accuracy: 0.990\n",
            "\n",
            "Training complete!\n",
            "0.99\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JgowBbTyhBlR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So we got accuracy of around 99% which is pretty cool"
      ]
    },
    {
      "metadata": {
        "id": "ajrH_ea5jcxW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### de nada!"
      ]
    }
  ]
}
