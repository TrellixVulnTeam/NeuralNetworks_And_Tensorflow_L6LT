{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the RNN in Tensorflow on PTB dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Prashant Brahmbhatt](https://www.github.com/hashbanger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "[Reference](https://github.com/adventuresinML)\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import collections\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Py3 = sys.version_info[0] == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_words(filename):\n",
    "    with tf.gfile.GFile(filename, \"r\") as f:\n",
    "        if Py3:\n",
    "            return f.read().replace('\\n', '<eos>').split()\n",
    "        else:\n",
    "            return f.read().decode('uft-8').replace('\\n','<eos>').split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_vocab(filename):\n",
    "    '''Input : Filename  \n",
    "    Return : Dictionary of word with their IDs as values'''\n",
    "    data = _read_words(filename)\n",
    "    #print(data)\n",
    "    counter = collections.Counter(data)\n",
    "    count_pairs = sorted(counter.items(), key = lambda x:(-x[1], x[0]))\n",
    "    \n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    \n",
    "    return word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _file_to_word_ids(filename, word_to_id):\n",
    "    ''' Input : filename, dictionary of words with their IDs  \n",
    "    Returns : list of all the IDs'''\n",
    "    data = _read_words(filename)\n",
    "    return [word_to_id[word] for word in data if word in word_to_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions first split the given text file into separate words and sentence based characters (i.e. end-of-sentence <eos>). Then, each unique word is identified and assigned a unique integer.  \n",
    "Finally, the original text file is converted into a list of these unique integers, where each word is substituted with its new integer identifier.  \n",
    "This allows the text data to be consumed in the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Batman',\n",
       " 'has',\n",
       " 'been',\n",
       " 'Gothams',\n",
       " 'protector',\n",
       " 'for',\n",
       " 'decades,',\n",
       " 'CEO',\n",
       " 'of',\n",
       " 'Wayne',\n",
       " 'Enterprises,',\n",
       " 'Patriarch',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Bat',\n",
       " 'Family',\n",
       " 'and',\n",
       " 'veteran',\n",
       " 'member',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Justice',\n",
       " 'League.',\n",
       " 'He',\n",
       " 'is',\n",
       " 'a',\n",
       " 'superhero',\n",
       " 'co-created',\n",
       " 'by',\n",
       " 'artist',\n",
       " 'Bob',\n",
       " 'Kane',\n",
       " 'and',\n",
       " 'writer',\n",
       " 'Bill',\n",
       " 'Finger',\n",
       " 'and',\n",
       " 'published',\n",
       " 'by',\n",
       " 'DC',\n",
       " 'Comics.',\n",
       " 'The',\n",
       " 'character',\n",
       " 'made',\n",
       " 'his',\n",
       " 'first',\n",
       " 'appearance',\n",
       " 'in',\n",
       " 'Detective',\n",
       " 'Comics.',\n",
       " 'Batman',\n",
       " 'is',\n",
       " 'the',\n",
       " 'secret',\n",
       " 'identity',\n",
       " 'of',\n",
       " 'Bruce',\n",
       " 'Wayne.',\n",
       " 'Witnessing',\n",
       " 'the',\n",
       " 'murder',\n",
       " 'of',\n",
       " 'his',\n",
       " 'parents',\n",
       " 'as',\n",
       " 'a',\n",
       " 'child',\n",
       " 'leads',\n",
       " 'him',\n",
       " 'to',\n",
       " 'train',\n",
       " 'himself',\n",
       " 'to',\n",
       " 'physical',\n",
       " 'and',\n",
       " 'intellectual',\n",
       " 'perfection',\n",
       " 'and',\n",
       " 'don',\n",
       " 'a',\n",
       " 'bat-themed',\n",
       " 'costume',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'fight',\n",
       " 'crime.',\n",
       " 'Batman',\n",
       " 'operates',\n",
       " 'in',\n",
       " 'Gotham',\n",
       " 'City,',\n",
       " 'assisted',\n",
       " 'by',\n",
       " 'various',\n",
       " 'supporting',\n",
       " 'characters',\n",
       " 'including',\n",
       " 'his',\n",
       " 'sidekick',\n",
       " 'Robin',\n",
       " 'and',\n",
       " 'his',\n",
       " 'butler',\n",
       " 'Alfred',\n",
       " 'Pennyworth,',\n",
       " 'and',\n",
       " 'fights',\n",
       " 'an',\n",
       " 'assortment',\n",
       " 'of',\n",
       " 'villains',\n",
       " 'influenced',\n",
       " 'by',\n",
       " 'the',\n",
       " \"characters'\",\n",
       " 'roots',\n",
       " 'in',\n",
       " 'film',\n",
       " 'and',\n",
       " 'pulp',\n",
       " 'magazines.',\n",
       " 'Unlike',\n",
       " 'most',\n",
       " 'superheroes,',\n",
       " 'he',\n",
       " 'does',\n",
       " 'not',\n",
       " 'possess',\n",
       " 'any',\n",
       " 'superpowers;',\n",
       " 'he',\n",
       " 'makes',\n",
       " 'use',\n",
       " 'of',\n",
       " 'intellect,',\n",
       " 'detective',\n",
       " 'skills,',\n",
       " 'science',\n",
       " 'and',\n",
       " 'technology,',\n",
       " 'wealth,',\n",
       " 'physical',\n",
       " 'prowess,',\n",
       " 'and',\n",
       " 'intimidation',\n",
       " 'in',\n",
       " 'his',\n",
       " 'war',\n",
       " 'on',\n",
       " 'crime.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_read_words('sample_text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Alfred': 14,\n",
       " 'Bat': 15,\n",
       " 'Batman': 6,\n",
       " 'Bill': 16,\n",
       " 'Bob': 17,\n",
       " 'Bruce': 18,\n",
       " 'CEO': 19,\n",
       " 'City,': 20,\n",
       " 'Comics.': 9,\n",
       " 'DC': 21,\n",
       " 'Detective': 22,\n",
       " 'Enterprises,': 23,\n",
       " 'Family': 24,\n",
       " 'Finger': 25,\n",
       " 'Gotham': 26,\n",
       " 'Gothams': 27,\n",
       " 'He': 28,\n",
       " 'Justice': 29,\n",
       " 'Kane': 30,\n",
       " 'League.': 31,\n",
       " 'Patriarch': 32,\n",
       " 'Pennyworth,': 33,\n",
       " 'Robin': 34,\n",
       " 'The': 35,\n",
       " 'Unlike': 36,\n",
       " 'Wayne': 37,\n",
       " 'Wayne.': 38,\n",
       " 'Witnessing': 39,\n",
       " 'a': 7,\n",
       " 'an': 40,\n",
       " 'and': 0,\n",
       " 'any': 41,\n",
       " 'appearance': 42,\n",
       " 'artist': 43,\n",
       " 'as': 44,\n",
       " 'assisted': 45,\n",
       " 'assortment': 46,\n",
       " 'bat-themed': 47,\n",
       " 'been': 48,\n",
       " 'butler': 49,\n",
       " 'by': 5,\n",
       " 'character': 50,\n",
       " 'characters': 51,\n",
       " \"characters'\": 52,\n",
       " 'child': 53,\n",
       " 'co-created': 54,\n",
       " 'costume': 55,\n",
       " 'crime.': 10,\n",
       " 'decades,': 56,\n",
       " 'detective': 57,\n",
       " 'does': 58,\n",
       " 'don': 59,\n",
       " 'fight': 60,\n",
       " 'fights': 61,\n",
       " 'film': 62,\n",
       " 'first': 63,\n",
       " 'for': 64,\n",
       " 'has': 65,\n",
       " 'he': 11,\n",
       " 'him': 66,\n",
       " 'himself': 67,\n",
       " 'his': 2,\n",
       " 'identity': 68,\n",
       " 'in': 3,\n",
       " 'including': 69,\n",
       " 'influenced': 70,\n",
       " 'intellect,': 71,\n",
       " 'intellectual': 72,\n",
       " 'intimidation': 73,\n",
       " 'is': 12,\n",
       " 'leads': 74,\n",
       " 'made': 75,\n",
       " 'magazines.': 76,\n",
       " 'makes': 77,\n",
       " 'member': 78,\n",
       " 'most': 79,\n",
       " 'murder': 80,\n",
       " 'not': 81,\n",
       " 'of': 1,\n",
       " 'on': 82,\n",
       " 'operates': 83,\n",
       " 'order': 84,\n",
       " 'parents': 85,\n",
       " 'perfection': 86,\n",
       " 'physical': 13,\n",
       " 'possess': 87,\n",
       " 'protector': 88,\n",
       " 'prowess,': 89,\n",
       " 'published': 90,\n",
       " 'pulp': 91,\n",
       " 'roots': 92,\n",
       " 'science': 93,\n",
       " 'secret': 94,\n",
       " 'sidekick': 95,\n",
       " 'skills,': 96,\n",
       " 'superhero': 97,\n",
       " 'superheroes,': 98,\n",
       " 'superpowers;': 99,\n",
       " 'supporting': 100,\n",
       " 'technology,': 101,\n",
       " 'the': 4,\n",
       " 'to': 8,\n",
       " 'train': 102,\n",
       " 'use': 103,\n",
       " 'various': 104,\n",
       " 'veteran': 105,\n",
       " 'villains': 106,\n",
       " 'war': 107,\n",
       " 'wealth,': 108,\n",
       " 'writer': 109}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_build_vocab('sample_text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6,\n",
       " 65,\n",
       " 48,\n",
       " 27,\n",
       " 88,\n",
       " 64,\n",
       " 56,\n",
       " 19,\n",
       " 1,\n",
       " 37,\n",
       " 23,\n",
       " 32,\n",
       " 1,\n",
       " 4,\n",
       " 15,\n",
       " 24,\n",
       " 0,\n",
       " 105,\n",
       " 78,\n",
       " 1,\n",
       " 4,\n",
       " 29,\n",
       " 31,\n",
       " 28,\n",
       " 12,\n",
       " 7,\n",
       " 97,\n",
       " 54,\n",
       " 5,\n",
       " 43,\n",
       " 17,\n",
       " 30,\n",
       " 0,\n",
       " 109,\n",
       " 16,\n",
       " 25,\n",
       " 0,\n",
       " 90,\n",
       " 5,\n",
       " 21,\n",
       " 9,\n",
       " 35,\n",
       " 50,\n",
       " 75,\n",
       " 2,\n",
       " 63,\n",
       " 42,\n",
       " 3,\n",
       " 22,\n",
       " 9,\n",
       " 6,\n",
       " 12,\n",
       " 4,\n",
       " 94,\n",
       " 68,\n",
       " 1,\n",
       " 18,\n",
       " 38,\n",
       " 39,\n",
       " 4,\n",
       " 80,\n",
       " 1,\n",
       " 2,\n",
       " 85,\n",
       " 44,\n",
       " 7,\n",
       " 53,\n",
       " 74,\n",
       " 66,\n",
       " 8,\n",
       " 102,\n",
       " 67,\n",
       " 8,\n",
       " 13,\n",
       " 0,\n",
       " 72,\n",
       " 86,\n",
       " 0,\n",
       " 59,\n",
       " 7,\n",
       " 47,\n",
       " 55,\n",
       " 3,\n",
       " 84,\n",
       " 8,\n",
       " 60,\n",
       " 10,\n",
       " 6,\n",
       " 83,\n",
       " 3,\n",
       " 26,\n",
       " 20,\n",
       " 45,\n",
       " 5,\n",
       " 104,\n",
       " 100,\n",
       " 51,\n",
       " 69,\n",
       " 2,\n",
       " 95,\n",
       " 34,\n",
       " 0,\n",
       " 2,\n",
       " 49,\n",
       " 14,\n",
       " 33,\n",
       " 0,\n",
       " 61,\n",
       " 40,\n",
       " 46,\n",
       " 1,\n",
       " 106,\n",
       " 70,\n",
       " 5,\n",
       " 4,\n",
       " 52,\n",
       " 92,\n",
       " 3,\n",
       " 62,\n",
       " 0,\n",
       " 91,\n",
       " 76,\n",
       " 36,\n",
       " 79,\n",
       " 98,\n",
       " 11,\n",
       " 58,\n",
       " 81,\n",
       " 87,\n",
       " 41,\n",
       " 99,\n",
       " 11,\n",
       " 77,\n",
       " 103,\n",
       " 1,\n",
       " 71,\n",
       " 57,\n",
       " 96,\n",
       " 93,\n",
       " 0,\n",
       " 101,\n",
       " 108,\n",
       " 13,\n",
       " 89,\n",
       " 0,\n",
       " 73,\n",
       " 3,\n",
       " 2,\n",
       " 107,\n",
       " 82,\n",
       " 10]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_file_to_word_ids('sample_text.txt', _build_vocab('sample_text.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    #getting the data paths\n",
    "    train_path = os.path.join(data_path, 'ptb.train.txt')\n",
    "    valid_path = os.path.join(data_path, 'ptb.valid.txt')\n",
    "    test_path = os.path.join(data_path, 'ptb.test.txt')\n",
    "\n",
    "    #build the complete vocabulary, then convert text data to list of integer\n",
    "    word_to_id = build_vocab(train_path)\n",
    "    train_data = _file_to_word_ids(train_path, word_to_id)\n",
    "    valid_data = _file_to_word_ids(valid_path, word_to_id)\n",
    "    test_data = _file_to_word_ids(test_path, word_to_id)\n",
    "    vocabulary = len(word_to_id)\n",
    "    reversed_dictionary = dict(zip(word_to_id.values(), word_to_id.keys()))\n",
    "    \n",
    "    print(train_data[:5])\n",
    "    print(word_to_id)\n",
    "    print(vocabulary)\n",
    "    print(\" \".join([reversed_dictionary[x] for x in train_data[:10]]))\n",
    "    \n",
    "    return train_data, valid_data, test_data, vocabulary, reversed_dictionary    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we setup the directory paths for the train, validation and test datasets and build_vocab() is invoked on the training data to create a dictionary that has each word as a key, and a unique integer as the associated value.  \n",
    "The reverse dictionary will allow us to go the other direction – from a unique integer identifier to the corresponding word.  \n",
    "This will be used later when we are reconstructing the outputs of our LSTM network back into plain English sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'E:\\Desarrollador\\Workspace\\Tensorflow_And_Neural_Networks\\Tensorflow\\Recurrent Neural Networks\\simple-examples\\data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Tensorflow, the use of a feed dictionary to supply data to the model during training, while common in tutorials, is not efficient. It is more efficient to use Tensorflow queues and threading. There's also ways of using Dataset API but we won't be applying here.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function batch_producer, which extracts batches of x, y training data – the x batch is formatted as the time stepped text data. The y batch is the same data, except delayed one time step. It looks like:  \n",
    "\n",
    "    x = “A girl walked into a bar, and she”\n",
    "    y = “girl walked into a bar, and she said”\n",
    "\n",
    "but these will be batches of integers rather than text data with size (batch_size, num_steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_producer(raw_data, batch_size, num_steps):\n",
    "    raw_data = tf.convert_to_tensor(raw_data, name = 'raw_data', dtype = tf.int32)\n",
    "    \n",
    "    data_len = tf.size(raw_data)\n",
    "    batch_len = data_len // batch_size\n",
    "    data = tf.reshape(raw_data[0: batch_size * batch_len], [batch_size , batch_len])\n",
    "    \n",
    "    epoch_size = (batch_len -1) // num_steps\n",
    "    \n",
    "    i = tf.train.range_input_producer(epoch_size, shuffle = False).dequeue()\n",
    "    x = data[:, i * num_steps:(i+1)* num_steps]\n",
    "    x.set_shape([batch_size, num_steps])\n",
    "    y = data[:, i * num_steps + 1:(i+1)* num_steps + 1]\n",
    "    y.set_shape([batch_size, num_steps])\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here first we have converted the raw text data into an int32 tensor.  \n",
    "The length of the full data set is calculated and stored in data_len and this is then divided by the batch size in an integer division (//) to get the number of full batches of data available within the dataset.   \n",
    "The next line reshapes the raw_data tensor (restricted in size to the number of full batches of data i.e. 0 to **(batch_size * batch_len)** into a **(batch_size, batch_len)** shape.   \n",
    "Then sets the number of iterations in each epoch – this is set so that all the training data is passed through the algorithm in each epoch.  \n",
    "This is what occurs here – the number of batches in the data **(batch_len)** is integer divided by the number of time steps – this gives the number of time-step-sized batches that are available to be iterated through in a single epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next line sets up an input range producer queue – this is a simple queue which allows the asynchronous and threaded extraction of data batches from a pre-existing dataset. Basically, each time more data is required in the training of the model, a new integer is extracted between 0 and epoch_size – this is then used in the following lines to extract a batch of data asynchronously from the data tensor. With the shuffle argument set to False, this integer simply cycles from 0 to epoch_size and then resets back at 0 to repeat.\n",
    "\n",
    "To produce the x, y batches of data, data slices are extracted from the data tensor based on the dequeued integer i.  \n",
    "It is easier to imagine a dummy dataset of integers up to 20 – [0, 1, 2, 3, 4, 5, 6, …, 19, 20]. We set the batch size to 3, and the number of steps to 2. The variables *batch_len* and *epoch_size* will therefore be equal to 6 and 2, respectively. The dummy reshaped data will look like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{bmatrix} \n",
    "1 & 2 & 3 & 4 & 5 & 6 \\\\ \n",
    "7 & 8 & 9 & 10 & 11 & 12 \\\\ \n",
    "13 & 14 & 15 & 16 & 17 & 18 \\\\ \n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first data batch extraction, i = 0, therefore the extracted x for our dummy dataset will be data[:, 0:2]  \n",
    "$$\\begin{bmatrix} \n",
    "1 & 2\\\\ \n",
    "7 & 8\\\\ \n",
    "13 & 14\\\\ \n",
    "\\end{bmatrix}$$  \n",
    "The extracted y will be data[:, 1:3]:  \n",
    "$$\\begin{bmatrix} \n",
    "2 & 3\\\\ \n",
    "8 & 9\\\\ \n",
    "14 & 15\\\\ \n",
    "\\end{bmatrix}$$  \n",
    "  \n",
    "Each row of the extracted x and y tensors will be an individual sample of length *num_steps* and the *number of rows* is the batch length. By organizing the data in this fashion, it is straight-forward to extract batch data while still maintaining the correct sentence sequence within each data sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a class, firstly, we pass this object important input data information such as *batch size*, the number of recurrent time steps and finally the raw data file we wish to extract batch data from.  \n",
    "Out previous *batch_producer* function, when called, will return our input data batch x and the associated time step + 1 target data batch, y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Input(object):\n",
    "    def __init__(self, batch_size, num_steps, data):\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.epoch_size = (len(data) // batch_size - 1) // num_steps\n",
    "        self.input_data , self.targets = batch_producer(data, batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the main model \n",
    "class Model(object):\n",
    "    def __init__(self, input, is_training, hidden_size, vocab_size, num_layers\n",
    "                 , dropout = 0.5, init_scale = 0.05):\n",
    "        self.is_training = is_training\n",
    "        self.input_obj = input\n",
    "        self.batch_size = input.batch_size\n",
    "        self.num_steps = input.num_steps\n",
    "        \n",
    "#------------------Creating Word Embeddings---------------------------------------------------------\n",
    "\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embedding = tf.Variable(tf.random_uniform([vocab_size, self.hidden_size] - init_scale, init_scale))\n",
    "            inputs = tf.nn.embedding_lookup(embedding, self.input_obj.input_data)\n",
    "\n",
    "#---------------------adding a dropout wrapper---------------------------------------------------\n",
    "\n",
    "        if is_training and dropout < 1:\n",
    "            inputs = tf.nn.dropout(inputs, dropout)\n",
    "            \n",
    "#------------setup the state stage / extraction-----------------------------------------------------\n",
    "\n",
    "        self.init_state = tf.placeholder(tf.float32, [num_layers, 2, self.batch_size, self.hidden_size])\n",
    "        \n",
    "        state_per_layer_list = tf.unstack(self.init_state, axis = 0)\n",
    "        rnn_tupled_state = tuple([tf.contrib.rnn.LSTMStateTuple(state_per_layer_list[idx][0]\n",
    "                                                                , state_per_layer_list[idx][1])\n",
    "                                 for idx in range(num_layers)])\n",
    "        \n",
    "#------------Creating an LSTM cell to be unrolled----------------------------------------------------\n",
    "        \n",
    "        cell = tf.contrib.rnn.LSTMCell(hidden_size, forget_bias = 1.0)\n",
    "        \n",
    "#------------Adding a dropout wrapper if training------------------------------------------------------\n",
    "\n",
    "        if is_training and dropout <1:\n",
    "            cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob = dropout)\n",
    "        if num_layers > 1:\n",
    "            cell = tf.contrib.rnn.MultiRNNCell([cell for _ in range(num_layers)], state_is_tuple = True)\n",
    "        \n",
    "#----------Creating Dynamic RNN object-----------------------------------------------------------\n",
    "\n",
    "        output, self.state = tf.nn.dynamic_rnn(cell, inputs, dtype= tf.float32, initial_state= rnn_tupled_state)\n",
    "\n",
    "######################################################################################################\n",
    "#---------------------Creating the softmax, loss and optimizer operations -----------------------------    \n",
    "######################################################################################################\n",
    "        \n",
    "        output = tf.reshape(output, [-1, hidden_size])\n",
    "        \n",
    "#---------setting up softmax weights and biases---------------------------------------------\n",
    "        \n",
    "        softmax_w = tf.Variable(tf.random_uniform([hidden_size, vocab_size], -init_scale, init_scale))\n",
    "        softmax_b = tf.Variable(tf.random_uniform([vocab_size], -init_scale, init_scale))\n",
    "        logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)\n",
    "        \n",
    "        #Reshape logits to be a 3-D tensor for sequence loss\n",
    "        logits = tf.reshape(logits, [self.batch_size, self.num_steps, vocab_size])\n",
    "\n",
    "#--------- Use the contrib sequence loss and average over the batches--------------------------\n",
    "\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(logits,  self.input_obj.targets\n",
    "                                                , tf.ones([self.batch_size, self.num_steps], dtype = tf.float32)\n",
    "                                                , average_across_timesteps = False\n",
    "                                                , average_across_batch = True)\n",
    "# update the cost\n",
    "        self.cost = tf.reduce_sum(loss)\n",
    "\n",
    "#---------get the prediction accuracy------------------------------------------------------------\n",
    "\n",
    "        self.softmax_out = tf.nn.softmax(tf.reshape(logits, [-1, vocab_size]))\n",
    "        self.predict = tf.cast(tf.argmax(self.softmax_out, axis = 1), tf.int32)\n",
    "        correct_prediction = tf.equal(self.predict, tf.reshape(self.input_obj.targets, [-1]))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "#------constructing the optimizers operations------------------------------------------------------\n",
    "\n",
    "        if not is_training:\n",
    "            return\n",
    "        self.learning_rate = tf.Variable(0.0, trainable= False)\n",
    "        \n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars), 5)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, tvars)\n",
    "                                                  , global_step = tf.contrib.framework.get_or_create_global_step())\n",
    "        \n",
    "        #updating the learning rate\n",
    "        self.new_lr = tf.placeholder(tf.float32, shape=[])\n",
    "        self.lr_update = tf.assign(self.learning_rate, self.new_lr)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now creating the main model.   \n",
    "The first part of initialization is pretty self-explanatory, with the input data information and batch producer operation found in input_obj. Another important input is the boolean is_training – this allows the model instance to be created either as a model setup for training, or alternatively setup for validation or testing only.  \n",
    "\n",
    "    class Model(object):\n",
    "        def __init__(self, input, is_training, hidden_size, vocab_size, num_layers\n",
    "                     , dropout = 0.5, init_scale = 0.05):\n",
    "            self.is_training = is_training\n",
    "            self.input_obj = input\n",
    "            self.batch_size = input.batch_size\n",
    "            self.num_steps = input.num_steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Creating the word embeddings.   \n",
    "Word embedding creates meaningful vectors to represent each word. First, we initialize the embedding variable with size **(vocab_size, hidden_size)** which creates the “lookup table” where each row represents a word in the dataset, and the set of columns is the embedding vector. In this case, our embedding vector length is set equal to the size of our LSTM hidden layer.  \n",
    "The next line performs a lookup action on the embedding tensor, where each word in the input data set is matched with a row in the embedding tensor, with the matched embedding vector being returned within inputs.  \n",
    "In this model, the embedding *layer / vectors* will be learned during the model training – however, if we so desired, we could also pre-learn embedding vectors using another model and upload these into our models.\n",
    "\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "            embedding = tf.Variable(tf.random_uniform([vocab_size, self.hidden_size] - init_scale, init_scale))\n",
    "            inputs = tf.nn.embedding_lookup(embedding, self.input_obj.input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The next step adds a drop-out wrapper to the input data – this helps prevent overfitting by continually changing the structure of the network connections  \n",
    "\n",
    "        if is_training and dropout < 1:\n",
    "            inputs = tf.nn.dropout(inputs, dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The next step is to setup the initial state TensorFlow placeholder.  \n",
    "This placeholder will be loaded with the initial state of the LSTM cells for each training batch. At the beginning of each training epoch, the input data will reset to the beginning of the text data set, so we want to reset the state variables to zero. However, during the multiple training batches executed in each epoch, we want to load the final state variables from the previous training batch into our LSTM cells for the current training batch. This keeps a certain continuity of state in our model, as we are progressing linearly through our text data set.  \n",
    "The second argument to the placeholder function is the size of the variable – (num_layers, 2, batch_size, hidden_size) and requires some explanation. If we consider an individual LSTM cell, for each training sample it processes it has two other inputs – the previous output from the cell (ht−1) and the previous state variable (st−1). These two inputs, h and s, are what is required to load the full state data into an LSTM cell. Remember also that h and s for each sample are actually vectors with the size equal to the hidden layer size. Therefore, for all the samples in the batch, for a single LSTM cell we have state data required of shape (2, batch_size, hidden_size). Finally, if we have stacked LSTM cell layers, we need state variables for each layer – num_layers. This gives the final shape of the state variables: (num_layers, 2, batch_size, hidden_size).  \n",
    "\n",
    "\n",
    "        self.init_state = tf.placeholder(tf.float32, [num_layers, 2, self.batch_size, self.hidden_size])  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The next two steps involve setting up this state data variable in the format required to feed it into the TensorFlow LSTM data structure:  \n",
    "\n",
    "\n",
    "        state_per_layer_list = tf.unstack(self.init_state, axis=0)\n",
    "        rnn_tuple_state = tuple(\n",
    "                    [tf.contrib.rnn.LSTMStateTuple(state_per_layer_list[idx][0],  state_per_layer_list[idx][1])\n",
    "                 for idx in range(num_layers)])\n",
    "\n",
    "The TensorFlow LSTM cell can accept the state as a tuple if a flag is set to True. The tf.unstack command creates a number of tensors, each of shape (2, batch_size, hidden_size), from the init_state tensor, one for each stacked LSTM layer (num_layer). These tensors are then loaded into a specific TensorFlow data structure, LSTMStateTuple, which is the required for input into the LSTM cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next we create an LSTM cell which will be “unrolled” over the number of time steps. Following this, we apply a drop-out wrapper to again protect against overfitting. Notice that we set the forget bias values to be equal to 1.0, which helps guard against repeated low forget gate outputs causing vanishing gradients  \n",
    "\n",
    "        # create an LSTM cell to be unrolled\n",
    "        cell = tf.contrib.rnn.LSTMCell(hidden_size, forget_bias=1.0)\n",
    "        # add a dropout wrapper if training\n",
    "        if is_training and dropout < 1:\n",
    "        cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we include many layers of stacked LSTM cells in the model, we need to use another TensorFlow object called MultiRNNCell which performs the requisite cell stacking / layering. We tell MultiRNNCell to expect the state variables in the form of a LSTMStateTuple by setting the flag state_is_tuple to True.   \n",
    "  \n",
    "        if num_layers > 1:\n",
    "            cell = tf.contrib.rnn.MultiRNNCell([cell for _ in range(num_layers)], state_is_tuple=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The final step in creating the LSTM network structure is to create a dynamic RNN object in TensorFlow. This object will dynamically perform the unrolling of the LSTM cell over each time step.\n",
    "\n",
    "        output, self.state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32, initial_state=rnn_tuple_state\n",
    "        \n",
    "    The dynamic_rnn object takes our defined LSTM cell as the first argument, and the embedding vector tensor inputs as the second argument. The final argument, initial_state is where we load our time-step zero state variables, that we created earlier, into the unrolled LSTM network.\n",
    "\n",
    "    This operation creates two outputs, the first is the output from all the unrolled LSTM cells, and will have a shape of (batch_size, num_steps, hidden_size). This data will be flattened in the next step to feed into a softmax classification layer. The second output, state, is the (s, h) state tuple taken from the final time step of the LSTM cells. This state operation / tuple will be extracted during each batch training operation to be used as inputs (via init_state) into the next training batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the softmax, loss and optimizer operations \n",
    "- Next we have to flatten the outputs so that we can feed them into our proposed softmax classification layer. We can use the -1 notation to reshape our output tensor, with the second axis set to be equal to the hidden layer size:\n",
    "\n",
    "        # reshape to (batch_size * num_steps, hidden_size)\n",
    "        output = tf.reshape(output, [-1, hidden_size])\n",
    "\n",
    "- Next we setup our softmax weight variables and the standard xw+b operation:\n",
    "\n",
    "        softmax_w = tf.Variable(tf.random_uniform([hidden_size, vocab_size], -init_scale, init_scale))\n",
    "        softmax_b = tf.Variable(tf.random_uniform([vocab_size], -init_scale, init_scale))\n",
    "        logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)\n",
    "\n",
    "    Note that the logits operation is simply the output of our tensor multiplication – we haven’t yet added the softmax operation – this will occur in the loss calculations below (and also in our ancillary accuracy calculations).\n",
    "\n",
    "- Following this, we have to setup our loss or cost function which will be used to train our LSTM network. In this case, we will use the specialized TensorFlow sequence to sequence loss function. This loss function allows one to calculate (a potentially) weighted cross entropy loss over a sequence of values.  \n",
    "    The first argument to this loss function is the logits argument, which requires tensors with the shape (batch_size, num_steps, vocab_size) – so we’ll need to reshape our logits tensor. The second argument to the loss function is the targets tensor which has a shape (batch_size, num_steps) with each value being an integer (which corresponds to a unique word in our case) – in other words, this tensor contains the true values of the word sequence that we want our LSTM network to predict. The third important argument is the weights tensor, of shape (batch_size, num_steps), which allows you to weight different samples or time steps with respect to the loss i.e. you might want the loss to favor the latter time steps rather than the earlier ones. No weighting is applied in this model, so a tensor of ones is passed to this argument.\n",
    "\n",
    "    There are two more important arguments for this function – average_across_timesteps and average_across_batch. If average_across_timesteps is set to True, the cost will be summed across the time dimension, if average_across_batch is True, then the cost will be summed across the batch dimension. In this case we are favoring the latter option.\n",
    "\n",
    "    Finally, we produce the cost operation which reduces the loss to a single scalar value – we could also do something similar by setting average_across_timesteps to True.\n",
    "\n",
    "        # Reshape logits to be a 3-D tensor for sequence loss\n",
    "        logits = tf.reshape(logits, [self.batch_size, self.num_steps, vocab_size])\n",
    "\n",
    "        # Use the contrib sequence loss and average over the batches\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(\n",
    "                    logits,\n",
    "                    self.input_obj.targets,\n",
    "                    tf.ones([self.batch_size, self.num_steps], dtype=tf.float32),\n",
    "                    average_across_timesteps=False,\n",
    "                    average_across_batch=True)\n",
    "        # Update the cost\n",
    "        self.cost = tf.reduce_sum(loss)\n",
    "\n",
    "\n",
    "\n",
    "In the next few steps, we set up some operations to calculate the accuracy off predictions over the batch samples:\n",
    "\n",
    "\n",
    "- First we apply a softmax operation to get the predicted probabilities of each word for each output of the LSTM network. We then make the network predictions equal to those words with the highest softmax probability by using the argmax function. These predictions are then compared to the actual target words and then averaged to get the accuracy.\n",
    "\n",
    "        # get the prediction accuracy\n",
    "        self.softmax_out = tf.nn.softmax(tf.reshape(logits, [-1, vocab_size]))\n",
    "        self.predict = tf.cast(tf.argmax(self.softmax_out, axis=1), tf.int32)\n",
    "        correct_prediction = tf.equal(self.predict, tf.reshape(self.input_obj.targets, [-1]))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "- Now we move onto constructing the optimization operations – in this case we aren’t using a simple “out of the box” optimizer – rather we are doing a few manipulations to improve results:\n",
    "    First off, if the model has been created for predictions, validations or testing only, these operations do not need to be created. The first step if the model is being used for training, is to create a learning rate variable. This will be used so that we can decrease the learning rate during training – this improves the final outcome of the model.\n",
    "\n",
    "    Next we wish to clip the size of the gradients in our network during back-propagation – this is recommended in recurrent neural networks to improve outcomes. Clipping values of between 1 and 5 are commonly used. Finally, we create the optimizer operation, using the learning_rate variable, and apply the clipped gradients.. Then a gradient descent step is performed – assigning this operation to train_op. This operation, train_op, will be called for each training batch.\n",
    "\n",
    "\n",
    "        if not is_training:\n",
    "           return\n",
    "        self.learning_rate = tf.Variable(0.0, trainable=False)\n",
    "\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars), 5)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "        self.train_op = optimizer.apply_gradients(\n",
    "                    zip(grads, tvars),\n",
    "                    global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "\n",
    "- The final two lines of the model creation involve the updating of the learning_rate:\n",
    "    First, a placeholder is created which will be input via the feed_dict argument when running the training, new_lr. This new learning rate is then assigned to learning_rate via a tf.assign operation. This operation, lr_update, will be run at the beginning of each epoch.\n",
    "\n",
    "        self.new_lr = tf.placeholder(tf.float32, shape=[])\n",
    "        self.lr_update = tf.assign(self.learning_rate, self.new_lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data, vocabulary, num_layers, num_epochs, batch_size, model_save_name\n",
    "          , learning_rate = 1.0, max_lr_epoch = 10, lr_decay = 0.93, print_iter = 50):\n",
    "    \n",
    "    #setting up data models\n",
    "    training_input = Input(batch_size= batch_size, num_steps= num_steps, data= train_data, )\n",
    "    m= Model(training_input, is_training= True, hidden_size= 650, vocab_size= vocabulary, num_layers= num_layers)\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    orig_decay = lr_decay\n",
    "    with tf.Session as sess:\n",
    "        \n",
    "        #start threads\n",
    "        sess.run([init_op])\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord= coord)\n",
    "        saver = tf.train.Saver()\n",
    "        for epoch in range(num_epochs):\n",
    "            new_lr_decay = orig_decay ** max(epoch + 1 - max_lr_epoch, 0.0)\n",
    "            m.assign_lr(sess, learning_rate * new_lr_decay)\n",
    "            current_state = np.zeros((num_layers, 2, batch_size, m.hidden_size))\n",
    "            curr_time = dt.datetime.now()\n",
    "            for step in range(training_input.epoch_size):\n",
    "                \n",
    "                if step % print_iter != 0:\n",
    "                    cost, _, current_state = sess.run([m.cost, m.train_op, m.state]\n",
    "                                                      , feed_dict = {m.init_state: current_state })\n",
    "                else:\n",
    "                    seconds = (float((dt.datetime.now() - curr_time).seconds) / print_iter)\n",
    "                    curr_time = dt.datetime.now()\n",
    "                    cost, _, current_state, acc = sess.run([m.cost, m.train_op, m.state, m.accuracy]\n",
    "                                                           , feed_dict = {m.init_state: current_state})\n",
    "                    print(\"Epoch {}, Step {}, Cost: {:.3f}, accuracy: {:.3f}, Seconds per step: {:.3f}\"\n",
    "                          .format(epoch, step, cost, acc, seconds))\n",
    "                    \n",
    "            #save a model checkpoint\n",
    "            saver.save(sess, data_path + '\\\\' + model_save_name, global_step= epoch)\n",
    "    #do a final save\n",
    "    saver.save(sess, data_path + '\\\\' + model_save_name + '-final')\n",
    "    #close threads\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training function will take as input the training data, along with various model parameters (batch sizes, number of steps etc.).  \n",
    "\n",
    "- First we create an Input object instance and a Model object instance, passing in the necessary parameters. Because the TensorFlow graph is being created during the initialization of these objects, the TensorFlow global variable initializer operation can only be properly run after the creation of these instances.\n",
    "\n",
    "        def train(train_data, vocabulary, num_layers, num_epochs, batch_size, model_save_name,\n",
    "                  learning_rate=1.0, max_lr_epoch=10, lr_decay=0.93):\n",
    "            # setup data and models\n",
    "            training_input = Input(batch_size=batch_size, num_steps=35, data=train_data)\n",
    "            m = Model(training_input, is_training=True, hidden_size=650, vocab_size=vocabulary,\n",
    "                      num_layers=num_layers)\n",
    "            init_op = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "- Next we start the session, and run the variable initializer operation. Because we are using queuing in the Input object, we also need to create a thread coordinator and start the running of the threads (for more information, see this tutorial). If you skip this step, or put it before the creation of training_input, your program will hang. Finally, a saver instance is created as we want to store model training checkpoints and the final trained model.\n",
    "\n",
    "        orig_decay = lr_decay\n",
    "        with tf.Session() as sess:\n",
    "            # start threads\n",
    "            sess.run([init_op])\n",
    "            coord = tf.train.Coordinator()\n",
    "            threads = tf.train.start_queue_runners(coord=coord)\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "- Next, the epochal training loop is entered into.  \n",
    "    The first step in every epoch is to calculate the learning rate decay factor, which gradually decreases after max_lr_epoch number of epochs has been reached. This learning rate decay factor, new_lr_decay, is multiplied by the learning rate and assigned to the model by calling the Model method assign_lr. This method looks like:\n",
    "\n",
    "        def assign_lr(self, session, lr_value):\n",
    "            session.run(self.lr_update, feed_dict={self.new_lr: lr_value})\n",
    "    \n",
    "    As can be observed, this function simply runs the lr_update operation.\n",
    "\n",
    "    The next step is to create a zeroed initial state tensor for our LSTM model – we assign this zeroed tensor to the variable current_state. Then each training operation is looped through within our specified epoch size. Every iteration we run the following operations: m.train_op and m.state. The train_op operation, as previously shown, calculates the clipped gradients of the model and takes a batched step to minimize the cost. The state operation returns the state of the final unrolled LSTM cell which we will require to input as the state for the next training batch – note that it replaces the contents of the current_state variable. This current_state variable is inserted into the m.init_state placeholder via the feed_dict.\n",
    "\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            new_lr_decay = orig_decay ** max(epoch + 1 - max_lr_epoch, 0.0)\n",
    "            m.assign_lr(sess, learning_rate * new_lr_decay)\n",
    "            current_state = np.zeros((num_layers, 2, batch_size, m.hidden_size))\n",
    "            for step in range(training_input.epoch_size):\n",
    "                if step % 50 != 0:\n",
    "                    cost, _, current_state = sess.run([m.cost, m.train_op, m.state],\n",
    "                                                                     feed_dict={m.init_state: current_state})\n",
    "                else:\n",
    "                    cost, _, current_state, acc = sess.run([m.cost, m.train_op, m.state, m.accuracy],\n",
    "                                                              feed_dict={m.init_state: current_state})\n",
    "                    print(\"Epoch {}, Step {}, cost: {:.3f}, accuracy: {:.3f}\".format(epoch, step, cost, acc))\n",
    "            # save a model checkpoint\n",
    "            saver.save(sess, data_path + '\\\\' + model_save_name, global_step=epoch)\n",
    "        # do a final save\n",
    "        saver.save(sess, data_path + '\\\\' + model_save_name + '-final')\n",
    "        # close threads\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every 50 iterations we also extract the current cost of the model in training, as well as the accuracy against the current training batch, to provide printed feedback during training. The outputs look like this:\n",
    "\n",
    "    Epoch 9, Step 1850, cost: 96.185, accuracy: 0.198\n",
    "    Epoch 9, Step 1900, cost: 94.755, accuracy: 0.235\n",
    "\n",
    "Finally, at the end of each epoch, we use the saver object to save a model checkpoint, and finally at the end of the training a final save of the state of the model is performed.\n",
    "\n",
    "The expected cost and accuracy progress through the epochs depends on the multitude of parameters supplied to the models and also the results of the random initialization of the variables. Training time is also dependent on whether you are using only CPUs, or whether you are using GPUs too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model_path, test_data, reversed_dictionary):\n",
    "    test_input = Input(batch_size=20, num_steps=35, data=test_data)\n",
    "    m = Model(test_input, is_training=False, hidden_size=650, vocab_size=vocabulary,\n",
    "              num_layers=2)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        # start threads\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        current_state = np.zeros((2, 2, m.batch_size, m.hidden_size))\n",
    "        # restore the trained model\n",
    "        saver.restore(sess, model_path)\n",
    "        # get an average accuracy over num_acc_batches\n",
    "        num_acc_batches = 30\n",
    "        check_batch_idx = 25\n",
    "        acc_check_thresh = 5\n",
    "        accuracy = 0\n",
    "        for batch in range(num_acc_batches):\n",
    "            if batch == check_batch_idx:\n",
    "                true_vals, pred, current_state, acc = sess.run([m.input_obj.targets, m.predict, m.state, m.accuracy],\n",
    "                                                               feed_dict={m.init_state: current_state})\n",
    "                pred_string = [reversed_dictionary[x] for x in pred[:m.num_steps]]\n",
    "                true_vals_string = [reversed_dictionary[x] for x in true_vals[0]]\n",
    "                print(\"True values (1st line) vs predicted values (2nd line):\")\n",
    "                print(\" \".join(true_vals_string))\n",
    "                print(\" \".join(pred_string))\n",
    "            else:\n",
    "                acc, current_state = sess.run([m.accuracy, m.state], feed_dict={m.init_state: current_state})\n",
    "            if batch >= acc_check_thresh:\n",
    "                accuracy += acc\n",
    "        print(\"Average accuracy: {:.3f}\".format(accuracy / (num_acc_batches-acc_check_thresh)))\n",
    "        # close threads\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with creating an Input and Model class that matches our training Input and Model classes. It is important that key parameters match the training model, such as the hidden size, number of steps, batch size etc. We are going to load our saved model variables into the computational graph created by the test Model instance, and if the dimensions don’t match TensorFlow will throw an error.\n",
    "\n",
    "Next we create a tf.train.Saver() operation – this will load all our saved model variables into our test model when we run the line saver.restore(sess, model_path). After dealing with all of the threads and creating a zeroed state variable, we setup some variables which relate to how we are going to assess the accuracy and look at some specific instances of predicted strings. Because we have to “warm up” the model by feeding it some data to get good state variables, we only measure the accuracy after a certain number of batches i.e. acc_check_thresh.\n",
    "\n",
    "When the batch number is equal to check_batch_idx the code runs the m.predict operation to extract the predictions for the particular batch of data. The first prediction of the batch is passed through the reverse dictionary to convert them back to actual words (along with the batch target words) and then compared with what should have been predicted via printing.\n",
    "\n",
    "Using the trained model, we can see the following output:\n",
    "\n",
    "True values (1st line) vs predicted values (2nd line):\n",
    "stock market is headed many traders were afraid to trust stock prices quoted on the big board <eos> the futures halt was even <unk> by big board floor traders <eos> it <unk> things up said\n",
    "market market is n’t for traders say willing to buy the prices <eos> <eos> the big board <eos> the dow market is a worse <eos> the board traders traders <eos> the ‘s the to to\n",
    "Average accuracy: 0.283\n",
    "\n",
    "The accuracy isn’t fantastic, but you can see the network is matching the “gist” of the sentence i.e. not producing all of the exact words but matching the general subject matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.data_path:\n",
    "    data_path = args.data_path\n",
    "train_data, valid_data, test_data, vocabulary, reversed_dictionary = load_data()\n",
    "if args.run_opt == 1:\n",
    "    train(train_data, vocabulary, num_layers=2, num_epochs=60, batch_size=20,\n",
    "          model_save_name='two-layer-lstm-medium-config-60-epoch-0p93-lr-decay-10-max-lr')\n",
    "else:\n",
    "    trained_model = args.data_path + \"\\\\two-layer-lstm-medium-config-60-epoch-0p93-lr-decay-10-max-lr-38\"\n",
    "test(trained_model, test_data, reversed_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### de nada!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
